#!/usr/bin/env python3
"""
å·¥å‚åˆçº¦æ£€æµ‹å™¨æœ‰æ•ˆæ€§è¯„ä¼°è„šæœ¬
ä½¿ç”¨Ground Truthæ•°æ®é›†è¯„ä¼°æ£€æµ‹å™¨çš„Precisionã€Recallç­‰æŒ‡æ ‡
"""

import json
import time
import logging
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from datetime import datetime
from sklearn.metrics import confusion_matrix, classification_report
import pandas as pd

# å¯¼å…¥æˆ‘ä»¬çš„æ£€æµ‹å™¨å’ŒGround Truthæ„å»ºå™¨
from factory_detector import ImprovedFactoryDetector
from ground_truth_builder import GroundTruthBuilder, GroundTruthContract, ContractType

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class EvaluationResult:
    """å•ä¸ªåˆçº¦çš„è¯„ä¼°ç»“æœ"""
    address: str
    ground_truth_label: bool
    predicted_label: bool
    factory_type_gt: Optional[str]
    factory_type_pred: Optional[str]
    confidence: Optional[float]
    analysis_time_ms: float
    is_correct: bool

@dataclass
class EvaluationMetrics:
    """è¯„ä¼°æŒ‡æ ‡"""
    # åŸºæœ¬æŒ‡æ ‡
    true_positives: int
    false_positives: int
    true_negatives: int 
    false_negatives: int
    
    # æ´¾ç”ŸæŒ‡æ ‡
    precision: float
    recall: float
    f1_score: float
    accuracy: float
    specificity: float
    
    # æ€§èƒ½æŒ‡æ ‡
    avg_analysis_time_ms: float
    total_analysis_time_ms: float

class DetectorEvaluator:
    """æ£€æµ‹å™¨è¯„ä¼°å™¨"""
    
    def __init__(self, detector=None, ground_truth_dataset=None):
        """åˆå§‹åŒ–è¯„ä¼°å™¨"""
        self.detector = detector or ImprovedFactoryDetector()
        self.ground_truth_dataset = ground_truth_dataset or []
        self.evaluation_results = []
        self.metrics = None
        
    def load_ground_truth_dataset(self, dataset_file: str):
        """åŠ è½½Ground Truthæ•°æ®é›†"""
        logger.info(f"Loading ground truth dataset from {dataset_file}")
        
        builder = GroundTruthBuilder()
        builder.load_dataset(dataset_file)
        self.ground_truth_dataset = builder.contracts
        
        logger.info(f"Loaded {len(self.ground_truth_dataset)} contracts for evaluation")
    
    def run_evaluation(self) -> List[EvaluationResult]:
        """è¿è¡Œå®Œæ•´è¯„ä¼°"""
        logger.info("Starting detector evaluation...")
        
        self.evaluation_results = []
        total_contracts = len(self.ground_truth_dataset)
        
        for i, contract in enumerate(self.ground_truth_dataset):
            logger.info(f"Evaluating contract {i+1}/{total_contracts}: {contract.address}")
            
            # è¿è¡Œæ£€æµ‹å™¨
            start_time = time.perf_counter()
            try:
                detection_result = self.detector.detect_factory_contract(contract.bytecode)
                analysis_time = (time.perf_counter() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                
                predicted_label = detection_result.is_factory_contract
                factory_type_pred = detection_result.factory_type if predicted_label else None
                confidence = getattr(detection_result, 'confidence', None)
                
            except Exception as e:
                logger.error(f"Error analyzing contract {contract.address}: {e}")
                analysis_time = (time.perf_counter() - start_time) * 1000
                predicted_label = False
                factory_type_pred = None
                confidence = None
            
            # åˆ›å»ºè¯„ä¼°ç»“æœ
            result = EvaluationResult(
                address=contract.address,
                ground_truth_label=contract.is_factory,
                predicted_label=predicted_label,
                factory_type_gt=contract.contract_type.value if contract.is_factory else None,
                factory_type_pred=factory_type_pred,
                confidence=confidence,
                analysis_time_ms=analysis_time,
                is_correct=(contract.is_factory == predicted_label)
            )
            
            self.evaluation_results.append(result)
            
            # æ¯50ä¸ªåˆçº¦æŠ¥å‘Šä¸€æ¬¡è¿›åº¦
            if (i + 1) % 50 == 0:
                accuracy_so_far = sum(1 for r in self.evaluation_results if r.is_correct) / len(self.evaluation_results)
                logger.info(f"Progress: {i+1}/{total_contracts}, Current accuracy: {accuracy_so_far:.3f}")
        
        logger.info("Evaluation completed!")
        return self.evaluation_results
    
    def calculate_metrics(self) -> EvaluationMetrics:
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡"""
        if not self.evaluation_results:
            raise ValueError("No evaluation results available. Run evaluation first.")
        
        logger.info("Calculating evaluation metrics...")
        
        # ç»Ÿè®¡TP, FP, TN, FN
        tp = sum(1 for r in self.evaluation_results 
                if r.ground_truth_label == True and r.predicted_label == True)
        fp = sum(1 for r in self.evaluation_results 
                if r.ground_truth_label == False and r.predicted_label == True)
        tn = sum(1 for r in self.evaluation_results 
                if r.ground_truth_label == False and r.predicted_label == False)
        fn = sum(1 for r in self.evaluation_results 
                if r.ground_truth_label == True and r.predicted_label == False)
        
        # è®¡ç®—æ´¾ç”ŸæŒ‡æ ‡
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1_score = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        accuracy = (tp + tn) / len(self.evaluation_results)
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
        
        # æ€§èƒ½æŒ‡æ ‡
        analysis_times = [r.analysis_time_ms for r in self.evaluation_results]
        avg_analysis_time = np.mean(analysis_times)
        total_analysis_time = sum(analysis_times)
        
        self.metrics = EvaluationMetrics(
            true_positives=tp,
            false_positives=fp,
            true_negatives=tn,
            false_negatives=fn,
            precision=precision,
            recall=recall,
            f1_score=f1_score,
            accuracy=accuracy,
            specificity=specificity,
            avg_analysis_time_ms=avg_analysis_time,
            total_analysis_time_ms=total_analysis_time
        )
        
        return self.metrics
    
    def print_metrics_report(self):
        """æ‰“å°è¯¦ç»†çš„æŒ‡æ ‡æŠ¥å‘Š"""
        if not self.metrics:
            self.calculate_metrics()
        
        print("\n" + "="*80)
        print("ğŸ­ FACTORY CONTRACT DETECTOR EVALUATION REPORT")
        print("="*80)
        
        # åŸºæœ¬ç»Ÿè®¡
        print(f"\nğŸ“Š BASIC STATISTICS:")
        print(f"Total contracts evaluated: {len(self.evaluation_results)}")
        print(f"Ground truth factories: {self.metrics.true_positives + self.metrics.false_negatives}")
        print(f"Ground truth non-factories: {self.metrics.true_negatives + self.metrics.false_positives}")
        
        # æ··æ·†çŸ©é˜µ
        print(f"\nğŸ¯ CONFUSION MATRIX:")
        print(f"                 Predicted")
        print(f"                 Factory  Non-Factory")
        print(f"Actual Factory    {self.metrics.true_positives:4d}      {self.metrics.false_negatives:4d}")
        print(f"    Non-Factory   {self.metrics.false_positives:4d}      {self.metrics.true_negatives:4d}")
        
        # æ€§èƒ½æŒ‡æ ‡
        print(f"\nğŸ“ˆ PERFORMANCE METRICS:")
        print(f"Precision:    {self.metrics.precision:.4f}")
        print(f"Recall:       {self.metrics.recall:.4f}")
        print(f"F1-Score:     {self.metrics.f1_score:.4f}")
        print(f"Accuracy:     {self.metrics.accuracy:.4f}")
        print(f"Specificity:  {self.metrics.specificity:.4f}")
        
        # æ€§èƒ½æ—¶é—´
        print(f"\nâ±ï¸  TIME PERFORMANCE:")
        print(f"Average analysis time: {self.metrics.avg_analysis_time_ms:.2f} ms")
        print(f"Total analysis time:   {self.metrics.total_analysis_time_ms/1000:.2f} seconds")
        print(f"Contracts per second:  {len(self.evaluation_results)/(self.metrics.total_analysis_time_ms/1000):.2f}")
    
    def analyze_errors(self) -> Dict[str, List[EvaluationResult]]:
        """åˆ†æé”™è¯¯æ¡ˆä¾‹"""
        logger.info("Analyzing error cases...")
        
        false_positives = [r for r in self.evaluation_results 
                          if not r.ground_truth_label and r.predicted_label]
        false_negatives = [r for r in self.evaluation_results 
                          if r.ground_truth_label and not r.predicted_label]
        
        print(f"\nâŒ ERROR ANALYSIS:")
        print(f"False Positives: {len(false_positives)}")
        print(f"False Negatives: {len(false_negatives)}")
        
        if false_positives:
            print(f"\nğŸ” FALSE POSITIVE EXAMPLES:")
            for i, fp in enumerate(false_positives[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"  {i+1}. {fp.address} (predicted as {fp.factory_type_pred})")
        
        if false_negatives:
            print(f"\nğŸ” FALSE NEGATIVE EXAMPLES:")
            for i, fn in enumerate(false_negatives[:5]):  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"  {i+1}. {fn.address} (actual type: {fn.factory_type_gt})")
        
        return {
            'false_positives': false_positives,
            'false_negatives': false_negatives
        }
    
    def analyze_by_contract_type(self):
        """æŒ‰åˆçº¦ç±»å‹åˆ†ææ€§èƒ½"""
        logger.info("Analyzing performance by contract type...")
        
        # æŒ‰Ground Truthç±»å‹åˆ†ç»„
        type_performance = {}
        
        for result in self.evaluation_results:
            gt_contract = next(c for c in self.ground_truth_dataset if c.address == result.address)
            contract_type = gt_contract.contract_type.value
            
            if contract_type not in type_performance:
                type_performance[contract_type] = {
                    'total': 0,
                    'correct': 0,
                    'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0
                }
            
            stats = type_performance[contract_type]
            stats['total'] += 1
            
            if result.is_correct:
                stats['correct'] += 1
            
            # æ›´æ–°æ··æ·†çŸ©é˜µç»Ÿè®¡
            if result.ground_truth_label and result.predicted_label:
                stats['tp'] += 1
            elif not result.ground_truth_label and result.predicted_label:
                stats['fp'] += 1
            elif not result.ground_truth_label and not result.predicted_label:
                stats['tn'] += 1
            else:  # ground_truth_label and not result.predicted_label
                stats['fn'] += 1
        
        print(f"\nğŸ“‹ PERFORMANCE BY CONTRACT TYPE:")
        print(f"{'Type':<25} {'Total':<6} {'Accuracy':<10} {'Precision':<10} {'Recall':<8}")
        print("-" * 65)
        
        for contract_type, stats in sorted(type_performance.items()):
            accuracy = stats['correct'] / stats['total'] if stats['total'] > 0 else 0
            precision = stats['tp'] / (stats['tp'] + stats['fp']) if (stats['tp'] + stats['fp']) > 0 else 0
            recall = stats['tp'] / (stats['tp'] + stats['fn']) if (stats['tp'] + stats['fn']) > 0 else 0
            
            print(f"{contract_type:<25} {stats['total']:<6} {accuracy:<10.3f} {precision:<10.3f} {recall:<8.3f}")
        
        return type_performance
    
    def generate_visualizations(self, output_dir: str = "./evaluation_plots"):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        import os
        os.makedirs(output_dir, exist_ok=True)
        
        logger.info(f"Generating visualizations in {output_dir}...")
        
        # è®¾ç½®å›¾è¡¨æ ·å¼
        plt.style.use('default')
        sns.set_palette("husl")
        
        # 1. æ··æ·†çŸ©é˜µçƒ­åŠ›å›¾
        fig, ax = plt.subplots(1, 1, figsize=(8, 6))
        
        y_true = [r.ground_truth_label for r in self.evaluation_results]
        y_pred = [r.predicted_label for r in self.evaluation_results]
        
        cm = confusion_matrix(y_true, y_pred)
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=['Non-Factory', 'Factory'],
                    yticklabels=['Non-Factory', 'Factory'], ax=ax)
        ax.set_title('Confusion Matrix')
        ax.set_ylabel('True Label')
        ax.set_xlabel('Predicted Label')
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/confusion_matrix.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. æ€§èƒ½æŒ‡æ ‡é›·è¾¾å›¾
        fig, ax = plt.subplots(1, 1, figsize=(8, 8), subplot_kw=dict(projection='polar'))
        
        metrics_names = ['Precision', 'Recall', 'F1-Score', 'Accuracy', 'Specificity']
        metrics_values = [
            self.metrics.precision,
            self.metrics.recall, 
            self.metrics.f1_score,
            self.metrics.accuracy,
            self.metrics.specificity
        ]
        
        angles = np.linspace(0, 2 * np.pi, len(metrics_names), endpoint=False).tolist()
        metrics_values += metrics_values[:1]  # é—­åˆå¤šè¾¹å½¢
        angles += angles[:1]
        
        ax.plot(angles, metrics_values, 'o-', linewidth=2, color='blue')
        ax.fill(angles, metrics_values, alpha=0.25, color='blue')
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics_names)
        ax.set_ylim(0, 1)
        ax.set_title('Performance Metrics Radar Chart')
        ax.grid(True)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/performance_radar.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        # 3. åˆ†ææ—¶é—´åˆ†å¸ƒç›´æ–¹å›¾
        fig, ax = plt.subplots(1, 1, figsize=(10, 6))
        
        analysis_times = [r.analysis_time_ms for r in self.evaluation_results]
        ax.hist(analysis_times, bins=50, alpha=0.7, color='green', edgecolor='black')
        ax.set_xlabel('Analysis Time (ms)')
        ax.set_ylabel('Frequency')
        ax.set_title('Distribution of Contract Analysis Times')
        ax.axvline(np.mean(analysis_times), color='red', linestyle='--', 
                  label=f'Mean: {np.mean(analysis_times):.2f} ms')
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f"{output_dir}/analysis_time_distribution.png", dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"âœ… Visualizations saved to {output_dir}/")
    
    def export_detailed_results(self, filename: str):
        """å¯¼å‡ºè¯¦ç»†çš„è¯„ä¼°ç»“æœ"""
        logger.info(f"Exporting detailed results to {filename}")
        
        # å‡†å¤‡å¯¼å‡ºæ•°æ®
        export_data = {
            'metadata': {
                'evaluation_date': datetime.now().isoformat(),
                'total_contracts': len(self.evaluation_results),
                'detector_version': getattr(self.detector, 'version', 'unknown')
            },
            'metrics': {
                'precision': self.metrics.precision,
                'recall': self.metrics.recall,
                'f1_score': self.metrics.f1_score,
                'accuracy': self.metrics.accuracy,
                'specificity': self.metrics.specificity,
                'confusion_matrix': {
                    'tp': self.metrics.true_positives,
                    'fp': self.metrics.false_positives,
                    'tn': self.metrics.true_negatives,
                    'fn': self.metrics.false_negatives
                },
                'performance': {
                    'avg_analysis_time_ms': self.metrics.avg_analysis_time_ms,
                    'total_analysis_time_ms': self.metrics.total_analysis_time_ms
                }
            },
            'detailed_results': [
                {
                    'address': r.address,
                    'ground_truth': r.ground_truth_label,
                    'predicted': r.predicted_label,
                    'gt_factory_type': r.factory_type_gt,
                    'pred_factory_type': r.factory_type_pred,
                    'confidence': r.confidence,
                    'analysis_time_ms': r.analysis_time_ms,
                    'correct': r.is_correct
                }
                for r in self.evaluation_results
            ]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… Detailed results exported to {filename}")

def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºè¯„ä¼°æµç¨‹"""
    print("ğŸ§ª Factory Contract Detector Evaluation")
    print("=" * 50)
    
    # åˆ›å»ºè¯„ä¼°å™¨
    evaluator = DetectorEvaluator()
    
    # æ¨¡æ‹Ÿï¼šåŠ è½½Ground Truthæ•°æ®é›†
    # å®é™…ä½¿ç”¨æ—¶ï¼Œè¯·æ›¿æ¢ä¸ºçœŸå®çš„æ•°æ®é›†æ–‡ä»¶
    print("ğŸ“ Loading ground truth dataset...")
    # evaluator.load_ground_truth_dataset("ground_truth_dataset_20241209_123456.json")
    
    # æ¼”ç¤ºæ¨¡å¼ï¼šåˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†
    from ground_truth_builder import GroundTruthContract, ContractType
    demo_contracts = []
    
    # æ·»åŠ ä¸€äº›æ¨¡æ‹Ÿçš„å·¥å‚åˆçº¦
    factory_bytecode = "0x608060405234801561001057600080fd5b5061012345678901234567890123456789012345678901234567890123456789abcdef"
    demo_contracts.append(GroundTruthContract(
        address="0x1111111111111111111111111111111111111111",
        chain="ethereum",
        is_factory=True,
        contract_type=ContractType.FACTORY_CREATE,
        bytecode=factory_bytecode,
        created_at=datetime.now(),
        block_number=18000000,
        tx_hash="0xaaaa",
        verification_method="known_project",
        verified_by="demo",
        verification_date=datetime.now(),
        confidence_level=1.0
    ))
    
    # æ·»åŠ ä¸€äº›æ¨¡æ‹Ÿçš„éå·¥å‚åˆçº¦
    token_bytecode = "0x608060405234801561001057600080fd5b50fedcba0987654321abcdef1234567890abcdef1234567890abcdef1234567890"
    demo_contracts.append(GroundTruthContract(
        address="0x2222222222222222222222222222222222222222", 
        chain="ethereum",
        is_factory=False,
        contract_type=ContractType.NON_FACTORY_TOKEN,
        bytecode=token_bytecode,
        created_at=datetime.now(),
        block_number=18000001,
        tx_hash="0xbbbb",
        verification_method="expert_manual",
        verified_by="demo",
        verification_date=datetime.now(),
        confidence_level=0.9
    ))
    
    evaluator.ground_truth_dataset = demo_contracts
    
    print(f"âœ… Loaded {len(evaluator.ground_truth_dataset)} contracts for evaluation")
    
    # è¿è¡Œè¯„ä¼°
    print("\nğŸš€ Running detector evaluation...")
    evaluator.run_evaluation()
    
    # è®¡ç®—å’Œæ˜¾ç¤ºæŒ‡æ ‡
    print("\nğŸ“Š Calculating metrics...")
    evaluator.calculate_metrics()
    evaluator.print_metrics_report()
    
    # é”™è¯¯åˆ†æ
    evaluator.analyze_errors()
    
    # æŒ‰ç±»å‹åˆ†æ
    evaluator.analyze_by_contract_type()
    
    # ç”Ÿæˆå¯è§†åŒ–ï¼ˆéœ€è¦å®‰è£…matplotlibå’Œseabornï¼‰
    try:
        evaluator.generate_visualizations()
    except ImportError:
        print("âš ï¸  Visualization libraries not available. Skipping plots.")
    
    # å¯¼å‡ºç»“æœ
    results_file = f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    evaluator.export_detailed_results(results_file)
    
    print(f"\nğŸ‰ Evaluation completed!")
    print(f"ğŸ“‹ Summary:")
    print(f"   - Precision: {evaluator.metrics.precision:.4f}")
    print(f"   - Recall:    {evaluator.metrics.recall:.4f}")
    print(f"   - F1-Score:  {evaluator.metrics.f1_score:.4f}")
    print(f"   - Accuracy:  {evaluator.metrics.accuracy:.4f}")
    print(f"\nğŸ“ Results saved to: {results_file}")

if __name__ == "__main__":
    main()