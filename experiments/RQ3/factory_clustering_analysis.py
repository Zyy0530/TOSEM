#!/usr/bin/env python3
"""
Factory Contract Clustering Analysis
å·¥å‚åˆçº¦èšç±»åˆ†æ - åŸºäºè¯­æ³•ã€è¯­ä¹‰ã€ç»“æ„ç‰¹å¾çš„å¤šç»´èšç±»
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.manifold import TSNE
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import re
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Set
import warnings
warnings.filterwarnings('ignore')

# Add parent directory to path for imports
sys.path.append('../..')
from ground_truth_dataset import GroundTruthDatasetBuilder

class FactoryContractFeatureExtractor:
    """å·¥å‚åˆçº¦ç‰¹å¾æå–å™¨"""
    
    def __init__(self):
        # é¢„å®šä¹‰çš„ä¸šåŠ¡é¢†åŸŸå…³é”®è¯
        self.business_keywords = {
            'finance': ['swap', 'pool', 'liquidity', 'lending', 'borrow', 'yield', 'farm', 'stake', 
                       'token', 'coin', 'defi', 'exchange', 'trade', 'price', 'fee', 'reward'],
            'nft': ['nft', 'token', 'metadata', 'uri', 'collection', 'mint', 'burn', 'transfer',
                   'erc721', 'erc1155', 'opensea', 'marketplace', 'royalty', 'game', 'art'],
            'infrastructure': ['proxy', 'implementation', 'beacon', 'registry', 'manager', 'admin',
                             'upgrade', 'delegate', 'forwarding', 'multicall', 'batch', 'utility'],
            'governance': ['dao', 'governance', 'proposal', 'vote', 'quorum', 'timelock', 'treasury',
                          'council', 'member', 'delegate', 'multisig', 'permission', 'role']
        }
        
        # æŠ€æœ¯æ¨¡å¼å…³é”®è¯
        self.technical_patterns = {
            'clone_factory': ['clone', 'clonefactory', 'minimal_proxy', 'eip1167'],
            'create2_factory': ['create2', 'deterministic', 'salt', 'predict'],
            'proxy_factory': ['proxy', 'implementation', 'beacon', 'upgrade'],
            'registry_factory': ['registry', 'register', 'mapping', 'directory']
        }
        
    def extract_features(self, source_code: str) -> Dict:
        """æå–å•ä¸ªåˆçº¦çš„å¤šç»´ç‰¹å¾"""
        features = {}
        source_lower = source_code.lower()
        
        # 1. è¯­æ³•ç‰¹å¾
        features.update(self._extract_syntactic_features(source_code, source_lower))
        
        # 2. è¯­ä¹‰ç‰¹å¾  
        features.update(self._extract_semantic_features(source_code, source_lower))
        
        # 3. ç»“æ„ç‰¹å¾
        features.update(self._extract_structural_features(source_code, source_lower))
        
        return features
    
    def _extract_syntactic_features(self, source_code: str, source_lower: str) -> Dict:
        """æå–è¯­æ³•ç‰¹å¾"""
        features = {}
        
        # å‡½æ•°å‘½åæ¨¡å¼
        create_functions = len(re.findall(r'function\s+create\w*', source_lower))
        deploy_functions = len(re.findall(r'function\s+deploy\w*', source_lower))
        new_statements = len(re.findall(r'new\s+\w+', source_lower))
        clone_functions = len(re.findall(r'function\s+clone\w*', source_lower))
        
        features.update({
            'create_functions': create_functions,
            'deploy_functions': deploy_functions, 
            'new_statements': new_statements,
            'clone_functions': clone_functions,
            'total_factory_methods': create_functions + deploy_functions + clone_functions
        })
        
        # ä¿®é¥°ç¬¦ä½¿ç”¨
        modifiers = ['onlyowner', 'payable', 'external', 'public', 'internal', 'private']
        for modifier in modifiers:
            features[f'has_{modifier}'] = 1 if modifier in source_lower else 0
            
        # äº‹ä»¶å®šä¹‰
        events = len(re.findall(r'event\s+\w+', source_lower))
        features['total_events'] = events
        
        return features
    
    def _extract_semantic_features(self, source_code: str, source_lower: str) -> Dict:
        """æå–è¯­ä¹‰ç‰¹å¾"""
        features = {}
        
        # ä¸šåŠ¡é¢†åŸŸç‰¹å¾
        for domain, keywords in self.business_keywords.items():
            domain_score = sum(1 for keyword in keywords if keyword in source_lower)
            features[f'{domain}_score'] = domain_score
            
        # æ ‡å‡†åº“ä½¿ç”¨
        standard_libs = ['erc20', 'erc721', 'erc1155', 'ownable', 'pausable', 'reentrancy']
        for lib in standard_libs:
            features[f'uses_{lib}'] = 1 if lib in source_lower else 0
            
        # OpenZeppelin å¯¼å…¥
        features['uses_openzeppelin'] = 1 if 'openzeppelin' in source_lower else 0
        
        return features
    
    def _extract_structural_features(self, source_code: str, source_lower: str) -> Dict:
        """æå–ç»“æ„ç‰¹å¾"""
        features = {}
        
        # CREATEæ“ä½œæ¨¡å¼
        features['has_create'] = 1 if 'create(' in source_lower else 0
        features['has_create2'] = 1 if 'create2(' in source_lower else 0
        
        # æŠ€æœ¯æ¨¡å¼
        for pattern, keywords in self.technical_patterns.items():
            pattern_score = sum(1 for keyword in keywords if keyword in source_lower)
            features[f'{pattern}_score'] = pattern_score
            
        # çŠ¶æ€ç®¡ç†å¤æ‚åº¦
        mappings = len(re.findall(r'mapping\s*\(', source_lower))
        arrays = len(re.findall(r'\[\]\s+', source_code))
        features['state_complexity'] = mappings + arrays
        
        # åˆçº¦å¤§å° (ä»£ç è¡Œæ•°çš„ä»£ç†)
        features['code_length'] = len(source_code)
        features['code_lines'] = source_code.count('\n')
        
        return features

class FactoryClusteringAnalyzer:
    """å·¥å‚åˆçº¦èšç±»åˆ†æå™¨"""
    
    def __init__(self):
        self.feature_extractor = FactoryContractFeatureExtractor()
        self.scaler = StandardScaler()
        
    def analyze_contracts(self, contracts_data: List[Dict]) -> Dict:
        """æ‰§è¡Œå®Œæ•´çš„èšç±»åˆ†æ"""
        print("ğŸ”¬ å¼€å§‹å·¥å‚åˆçº¦èšç±»åˆ†æ...")
        
        # 1. ç‰¹å¾æå–
        print("ğŸ“Š æå–åˆçº¦ç‰¹å¾...")
        features_df = self._extract_all_features(contracts_data)
        
        # 2. ç‰¹å¾é¢„å¤„ç†
        print("ğŸ”§ ç‰¹å¾é¢„å¤„ç†å’Œæ ‡å‡†åŒ–...")
        X_scaled = self._preprocess_features(features_df)
        
        # 3. ç¡®å®šæœ€ä¼˜èšç±»æ•°
        print("ğŸ¯ ç¡®å®šæœ€ä¼˜èšç±»æ•°...")
        optimal_k = self._find_optimal_clusters(X_scaled)
        
        # 4. æ‰§è¡Œèšç±»
        print(f"ğŸª æ‰§è¡ŒK-Meansèšç±» (k={optimal_k})...")
        cluster_labels = self._perform_clustering(X_scaled, optimal_k)
        
        # 5. èšç±»ç»“æœè§£é‡Š
        print("ğŸ” åˆ†æèšç±»ç»“æœ...")
        cluster_analysis = self._analyze_clusters(features_df, cluster_labels, contracts_data)
        
        # 6. å¯è§†åŒ–
        print("ğŸ“ˆ ç”Ÿæˆèšç±»å¯è§†åŒ–...")
        self._visualize_clusters(X_scaled, cluster_labels, cluster_analysis)
        
        return {
            'features_df': features_df,
            'cluster_labels': cluster_labels,
            'cluster_analysis': cluster_analysis,
            'optimal_k': optimal_k
        }
    
    def _extract_all_features(self, contracts_data: List[Dict]) -> pd.DataFrame:
        """æå–æ‰€æœ‰åˆçº¦çš„ç‰¹å¾"""
        all_features = []
        
        for i, contract in enumerate(contracts_data):
            if i % 50 == 0:
                print(f"  å¤„ç†è¿›åº¦: {i}/{len(contracts_data)}")
                
            try:
                features = self.feature_extractor.extract_features(contract['source_code'])
                features['address'] = contract['address']
                all_features.append(features)
            except Exception as e:
                print(f"  è­¦å‘Š: å¤„ç†åˆçº¦ {contract['address']} æ—¶å‡ºé”™: {e}")
                continue
                
        return pd.DataFrame(all_features)
    
    def _preprocess_features(self, features_df: pd.DataFrame) -> np.ndarray:
        """ç‰¹å¾é¢„å¤„ç†"""
        # ç§»é™¤éæ•°å€¼ç‰¹å¾
        numeric_features = features_df.select_dtypes(include=[np.number])
        
        # å¤„ç†ç¼ºå¤±å€¼
        numeric_features = numeric_features.fillna(0)
        
        # æ ‡å‡†åŒ–
        X_scaled = self.scaler.fit_transform(numeric_features)
        
        return X_scaled
    
    def _find_optimal_clusters(self, X: np.ndarray, max_k: int = 10) -> int:
        """ä½¿ç”¨è‚˜éƒ¨æ³•å’Œè½®å»“ç³»æ•°ç¡®å®šæœ€ä¼˜èšç±»æ•°"""
        inertias = []
        silhouette_scores = []
        k_range = range(2, min(max_k + 1, len(X) // 2))
        
        for k in k_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(X)
            
            inertias.append(kmeans.inertia_)
            silhouette_scores.append(silhouette_score(X, cluster_labels))
        
        # é€‰æ‹©è½®å»“ç³»æ•°æœ€é«˜çš„k
        optimal_k = k_range[np.argmax(silhouette_scores)]
        
        return optimal_k
    
    def _perform_clustering(self, X: np.ndarray, k: int) -> np.ndarray:
        """æ‰§è¡ŒK-Meansèšç±»"""
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(X)
        
        return cluster_labels
    
    def _analyze_clusters(self, features_df: pd.DataFrame, cluster_labels: np.ndarray, 
                         contracts_data: List[Dict]) -> Dict:
        """åŠ¨æ€åˆ†æèšç±»ç»“æœå¹¶è‡ªåŠ¨ç¡®å®šç±»å‹"""
        features_df['cluster'] = cluster_labels
        
        cluster_analysis = {}
        
        for cluster_id in np.unique(cluster_labels):
            cluster_data = features_df[features_df['cluster'] == cluster_id]
            
            # è®¡ç®—æ¯ä¸ªèšç±»çš„ç‰¹å¾å‡å€¼
            numeric_features = cluster_data.select_dtypes(include=[np.number])
            feature_means = numeric_features.mean()
            
            # æ‰¾å‡ºæœ€æ˜¾è‘—çš„ç‰¹å¾
            top_features = feature_means.nlargest(10)
            
            # åŠ¨æ€ç¡®å®šèšç±»ç±»å‹
            cluster_type_info = self._determine_cluster_type_dynamically(feature_means, top_features)
            
            cluster_analysis[cluster_id] = {
                'size': len(cluster_data),
                'top_features': top_features.to_dict(),
                'cluster_type': cluster_type_info,
                'sample_addresses': cluster_data['address'].head(3).tolist()
            }
            
        return cluster_analysis
    
    def _determine_cluster_type_dynamically(self, feature_means: pd.Series, top_features: pd.Series) -> Dict:
        """åŸºäºç‰¹å¾åŠ¨æ€ç¡®å®šèšç±»ç±»å‹"""
        
        # 1. æŠ€æœ¯æ¨¡å¼è¯†åˆ« (åŸºäºæŠ€æœ¯ç‰¹å¾çš„é˜ˆå€¼)
        technical_patterns = {}
        if feature_means.get('clone_factory_score', 0) > 1.0:
            technical_patterns['Clone Factory'] = feature_means['clone_factory_score']
        if feature_means.get('create2_factory_score', 0) > 1.0:
            technical_patterns['CREATE2 Factory'] = feature_means['create2_factory_score']
        if feature_means.get('proxy_factory_score', 0) > 1.0:
            technical_patterns['Proxy Factory'] = feature_means['proxy_factory_score']
        if feature_means.get('registry_factory_score', 0) > 1.0:
            technical_patterns['Registry Factory'] = feature_means['registry_factory_score']
        
        # ç¡®å®šä¸»è¦æŠ€æœ¯æ¨¡å¼
        primary_technical = max(technical_patterns, key=technical_patterns.get) if technical_patterns else "Standard Factory"
        
        # 2. ä¸šåŠ¡é¢†åŸŸè¯†åˆ« (åŸºäºä¸šåŠ¡ç‰¹å¾çš„ç›¸å¯¹å¼ºåº¦)
        business_scores = {
            'Finance': feature_means.get('finance_score', 0),
            'NFT': feature_means.get('nft_score', 0),
            'Infrastructure': feature_means.get('infrastructure_score', 0),
            'Governance': feature_means.get('governance_score', 0)
        }
        
        # è®¡ç®—ä¸šåŠ¡é¢†åŸŸçš„æ˜¾è‘—æ€§ (ç›¸å¯¹äºå¹³å‡æ°´å¹³)
        total_business_score = sum(business_scores.values())
        if total_business_score > 0:
            business_distribution = {k: v/total_business_score for k, v in business_scores.items()}
            primary_business = max(business_distribution, key=business_distribution.get)
            business_confidence = business_distribution[primary_business]
        else:
            primary_business = "Generic"
            business_confidence = 0.25
        
        # 3. ç‰¹æ®Šæ¨¡å¼æ£€æµ‹
        special_patterns = []
        if feature_means.get('has_create2', 0) > 0.7:
            special_patterns.append("Deterministic Deployment")
        if feature_means.get('uses_openzeppelin', 0) > 0.8:
            special_patterns.append("OpenZeppelin Based")
        if feature_means.get('total_factory_methods', 0) > 3:
            special_patterns.append("Multi-Method Factory")
        if feature_means.get('state_complexity', 0) > 5:
            special_patterns.append("Complex State Management")
        
        # 4. åŠ¨æ€ç”Ÿæˆç±»å‹æ ‡ç­¾
        if business_confidence > 0.6:  # ä¸šåŠ¡é¢†åŸŸæ˜ç¡®
            semantic_type = f"{primary_business} {primary_technical}"
        elif technical_patterns:  # æŠ€æœ¯æ¨¡å¼æ˜ç¡®
            semantic_type = f"{primary_technical} Pattern"
        else:  # é€šç”¨æ¨¡å¼
            if special_patterns:
                semantic_type = f"Generic Factory with {', '.join(special_patterns[:2])}"
            else:
                semantic_type = "Standard Factory Pattern"
        
        return {
            'semantic_type': semantic_type,
            'primary_technical': primary_technical,
            'primary_business': primary_business,
            'business_confidence': business_confidence,
            'technical_patterns': technical_patterns,
            'special_patterns': special_patterns,
            'business_distribution': business_scores
        }
    
    def _visualize_clusters(self, X: np.ndarray, cluster_labels: np.ndarray, 
                           cluster_analysis: Dict):
        """ç”Ÿæˆèšç±»å¯è§†åŒ–å›¾"""
        # ä½¿ç”¨t-SNEè¿›è¡Œé™ç»´
        print("  æ‰§è¡Œt-SNEé™ç»´...")
        tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(X)//4))
        X_tsne = tsne.fit_transform(X)
        
        # å®šä¹‰èšç±»æ ‡ç­¾å’Œé¢œè‰²
        cluster_names = []
        for cluster_id in np.unique(cluster_labels):
            domain = cluster_analysis[cluster_id]['dominant_domain']
            cluster_names.append(f"Cluster {cluster_id}: {domain.title()}")
        
        # åˆ›å»ºå›¾å½¢
        plt.figure(figsize=(12, 8))
        
        # è®¾ç½®é¢œè‰²
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']
        markers = ['o', '^', 's', 'D', 'v', '<', '>', 'p', '*', 'h']
        
        # ç»˜åˆ¶æ¯ä¸ªèšç±»
        for i, cluster_id in enumerate(np.unique(cluster_labels)):
            mask = cluster_labels == cluster_id
            plt.scatter(X_tsne[mask, 0], X_tsne[mask, 1], 
                       c=colors[i % len(colors)], 
                       marker=markers[i % len(markers)],
                       alpha=0.7, s=60,
                       label=cluster_names[i])
        
        plt.xlabel('Dimension 1 After Dimensionality reduction', fontsize=12)
        plt.ylabel('Dimension 2 After Dimensionality reduction', fontsize=12)
        plt.title('Clustered Semantic Space', fontsize=16, fontweight='bold')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        
        # ä¿å­˜å›¾å½¢
        output_path = '/Users/mac/ResearchSpace/TOSEM/experiments/RQ3/factory_clustering_result.pdf'
        plt.savefig(output_path, dpi=300, bbox_inches='tight')
        print(f"  èšç±»å›¾å·²ä¿å­˜åˆ°: {output_path}")
        
        plt.show()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ­ å·¥å‚åˆçº¦èšç±»åˆ†æ")
    print("=" * 50)
    
    # 1. ä»BigQueryè·å–å·¥å‚åˆçº¦æ•°æ®
    print("ğŸ“¥ ä»BigQueryè·å–å·¥å‚åˆçº¦æ•°æ®...")
    
    # è¿™é‡Œéœ€è¦å®ç°æ•°æ®è·å–é€»è¾‘
    # contracts_data = get_factory_contracts_from_bigquery()
    
    # ä¸ºäº†æµ‹è¯•ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸€äº›ç¤ºä¾‹æ•°æ®
    contracts_data = generate_sample_data()
    
    # 2. æ‰§è¡Œèšç±»åˆ†æ
    analyzer = FactoryClusteringAnalyzer()
    results = analyzer.analyze_contracts(contracts_data)
    
    # 3. ç”ŸæˆæŠ¥å‘Š
    generate_analysis_report(results)
    
    print("âœ… èšç±»åˆ†æå®Œæˆï¼")

def generate_sample_data() -> List[Dict]:
    """ç”Ÿæˆç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•"""
    # è¿™é‡Œåº”è¯¥æ›¿æ¢ä¸ºçœŸå®çš„BigQueryæ•°æ®è·å–
    return [
        {
            'address': '0x1234...', 
            'source_code': '''
            contract UniswapV2Factory {
                function createPair(address tokenA, address tokenB) external returns (address pair) {
                    // DEX factory logic
                    pair = new UniswapV2Pair();
                }
            }'''
        },
        # æ·»åŠ æ›´å¤šç¤ºä¾‹...
    ]

def generate_analysis_report(results: Dict):
    """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
    print("\nğŸ“Š èšç±»åˆ†ææŠ¥å‘Š:")
    print("=" * 30)
    
    for cluster_id, analysis in results['cluster_analysis'].items():
        print(f"\nèšç±» {cluster_id} ({analysis['dominant_domain'].title()}):")
        print(f"  åˆçº¦æ•°é‡: {analysis['size']}")
        print(f"  ä¸»è¦ç‰¹å¾: {list(analysis['top_features'].keys())[:3]}")
        print(f"  ç¤ºä¾‹åœ°å€: {analysis['sample_addresses']}")

if __name__ == "__main__":
    main()